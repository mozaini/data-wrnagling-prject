{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dea38435",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    ">In this report, I will be describing the effort I have made regarding the wrangling process on the project of Wrangling and Analyzing WeRateDogs Twitter Account (DAND project). \n",
    "\n",
    ">I started the wrangling process by gathering the needed data from different sources. The data files were separate and in three different formats. Two datasets files were uploaded on Udacity servers, and the other data I had to extract directly from the <a href=\"https://twitter.com/dog_rates\">@dog_rates</a> Twitter account using an API Called Tweetpy. I then went to assess the three datasets visually and programmatically, searching for issues that needed to be addressed and fixed regarding the quality the tidiness of the datasets. After evaluating the datasets, I dealt with all the problems marked in the assessment process and handled them one by one. Lastly, I analyzed the prepared dataset looking for meaningful insights and visuals. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f554e174",
   "metadata": {},
   "source": [
    "# Part I - Data Gathering\n",
    "\n",
    "> Gathering the Pieces of datasets required for the project. In our case, three datasets needed to be fetched and downloaded into the notebook\n",
    "> The three datasets were:\n",
    ">    - WeRateDogs Twitter account archive, which was hosted on Udacity server on this <a herf=\"https://d17h27t6h515a5.cloudfront.net/topher/2017/August/59a4e958_twitter-archive-enhanced/twitter-archive-enhanced.csv\"> link</a> as `csv` format. I used the Python requests library to download the dataset.\n",
    ">    - The tweet image predictions, which was also hosted on Udacity server on this <a herf = \"https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\"> link</a> formatted as `tsv`. I utilized the Python requests library as well to download the dataset.\n",
    ">    - The other data I have gathered was the tweets' retweet counts and favorite counts for each tweet in the two datasets I gathered in the previous steps. I used tweetpy API to go through WeRateDogs twitter's account, extracted the retweet and favorite counts of each tweet, and stored them in a new dataset.\n",
    "\n",
    "\n",
    "\n",
    "> After gathering the data I needed, I ended up with three datasets:\n",
    "    > - The WeRateDogs Twitter archive, which contains the tweets' text combined with some basic information about the tweets, such as tweet id, ratings, dog stages, names, etc...  \n",
    "    > - Image Predictions File, a table full of dog breeds predictions alongside tweet IDs and image URLs. \n",
    "    > - A dataset with retweets' and likes' counts alongside their tweet ids. Collecting retweets and likes counts required querying the tweets ids from The WeRateDogs Twitter archive using tweetpy.api.get_status(), which returned json data containing detailed information about each tweet, including retweet's counts and likes' counts. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc717f51",
   "metadata": {},
   "source": [
    "# Part II - Data Assessment \n",
    "\n",
    "\n",
    "> - After I gathered all the data I needed for the project, I started assessing the three datasets regarding the quality of each dataset and the tidiness of tables.\n",
    ">- I began assessing The WeRateDogs Twitter archive dataset, in which I have found many tidiness and quality issues, some of which are: \n",
    ">> - Missing values in columns `reply_to_status_id`, `in_reply_to_user_id`, `retweeted_status_id`\n",
    ">> - Null values are represented as a string 'None' instead of `NaN.`\n",
    ">> - Invalid `rating_numerator` and `rating_denominator`\n",
    ">> - improper datatypes, such as `timestamp` should be DateTime instated of a Pandas object\n",
    ">> - Inappropriate names in the dog's name column [\"a\", \"the\", \"None\",'O','all','old']\n",
    ">> - There 137 duplicates\n",
    ">> - Tweets' texts contained extra words regarding the status of the tweet, retweet or original tweet, and links.\n",
    ">> - `source` had HTML elements and tags that were removed ultimately. \n",
    ">> - columns that were uncalled for, such as (`retweeted_status_id`,`retweeted_status_user_id`,`retweeted_status_timestamp`)\n",
    ">> - `doggo` `floofer` `pupper` `puppo` were melted in one Column called dog_stage, converted them into long format instead of the wide-format\n",
    "> - The image predictions dataset also had some quilty issues that had been inspected, naming:\n",
    ">> - Words were separated with underscore signs `_` instead of just space in the columns (`p1`, `p2`, `p3`)\n",
    ">> - Inconsistent capitalization in the columns (`p1`, `p2`,`p3`)\n",
    ">> - wrong prediction values in the columns (`p1`, `p2`, `p3`) due to distractions in the images, missing images, or unreal dog pictures. \n",
    ">> - 66 duplicates in `jpg_url` column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11939484",
   "metadata": {},
   "source": [
    "# Part III - Data Cleanign\n",
    "\n",
    "> - The last step of the wrangling process was cleaning the data and fixing the issues recognized during the data assessment process.\n",
    "> - The first step was combining the three tables we already have into one dataset, then making a copy of the dataset since we wanted to keep the original file untouched in case we wanted to go back and use it. \n",
    "> - To make the cleaning process organized and clear, I have utilized the Define-Code-Test approach. It's an approach in which I start to define the cleaning issue I would be fixing first. Then, I moved to handle the defined problem using pandas or whatever python library or method. After I fixed the problem, I tested my code under the testing section to ensure that what I did in the code section was a success."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
